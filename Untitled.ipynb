{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc2e874",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ashrafwkhalil/NN-Poetry-Generator/blob/main/Untitled.ipynb#scrollTo=cba778ac\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a633e",
   "metadata": {},
   "source": [
    "# Poem Generator\n",
    "I will create a Poem Generator by training a neural network to predict the next word given an input sequence of words, and then using that model to continuously generate text. The model will be trained on a large dataset of poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98676ad7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae36ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8b14c",
   "metadata": {},
   "source": [
    "Here I will be downloading the entire github repo so that I can access all neccessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone 'https://github.com/ashrafwkhalil/NN-Poetry-Generator.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802e1dd",
   "metadata": {},
   "source": [
    "## Organize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d90fca",
   "metadata": {},
   "source": [
    "In the below cell, I will be formatting my poem data into just lines of text with some maximum length, measured in number of words. I have stored the max line length value as a variable, thus allowing me to use it almost as a hyperparameter, as I am able to modify it and compare model performance with different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Poems list\n",
    "poems = []\n",
    "# Poems here are stored in many different directories, I am going to iterate through all the different directories\n",
    "# and simply append all poems to the same list. This may be problematic, given that all of these directories represent\n",
    "# different styles of poems, but I will overlook that for this demo.\n",
    "forms_poems_path = './Poems/forms'\n",
    "topics_poems_path = './Poems/topics'\n",
    "directories_forms = os.listdir(forms_poems_path)\n",
    "directories_topics = os.listdir(topics_poems_path)\n",
    "# iterating through all directories\n",
    "for poem_form in directories_forms:\n",
    "    try:\n",
    "        for poem_file in os.listdir(os.path.join(forms_poems_path, poem_form)):\n",
    "            with open(os.path.join(forms_poems_path, poem_form, poem_file)) as poem:\n",
    "                lines = poem.readlines()\n",
    "                lines_newline_spaced = []\n",
    "                for line in lines:\n",
    "                    line = line.replace('\\n', ' \\n')\n",
    "                    lines_newline_spaced.append(line)\n",
    "                poems.append(lines_newline_spaced)[1:-1]\n",
    "    except:\n",
    "        continue\n",
    "for poem_form in directories_topics:\n",
    "    try:\n",
    "        for poem_file in os.listdir(os.path.join(topics_poems_path, poem_form)):\n",
    "            with open(os.path.join(topics_poems_path, poem_form, poem_file)) as poem:\n",
    "                lines = poem.readlines()\n",
    "                lines_newline_spaced = []\n",
    "                for line in lines:\n",
    "                    line = line.replace('\\n', ' \\n')\n",
    "                    lines_newline_spaced.append(line)\n",
    "                poems.append(lines_newline_spaced)[1:-1]\n",
    "    except:\n",
    "        continue\n",
    "# the max_line_length variable. This will decide what the maximum length of a single input into the model will be. \n",
    "# Any line above the max line length will be split up into separate elements, and these elements will be inserted \n",
    "# into the array\n",
    "max_line_length = 20\n",
    "poems = np.concatenate(poems)\n",
    "for i, line in enumerate(poems):\n",
    "    # split into arrays of words\n",
    "    line = line.split(' ')\n",
    "    if len(line) > max_line_length:\n",
    "        # add first maxlinelength sized chunk into array at original index\n",
    "        poems[i] = ' '.join(line[:max_line_length])\n",
    "        #split iterate through chunks of max line length\n",
    "        for x in range(1, int(len(line)/max_line_length)):\n",
    "            #insert chunks into poem list\n",
    "            np.insert(poems, i+x, ' '.join(line[max_line_length*(i):max_line_length*(i+1)]))\n",
    "poems[95:145]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0148a7e9",
   "metadata": {},
   "source": [
    "## Tokenizing the inputs\n",
    "Here, I will tokenize all the inputs, convert them into tokenized sequences, then break down a single sequence into multiple. I will do this by repeatedly removing a word from the end of the sequence and using this new sequence, with the last word removed, as a separate sequence. The minimum size for a sequence at this stage will be 2 words long. In the next step, I will use the last word in a given sequence as the label for that sequence, thus making it so at least 2 words is necessary. I also add sequences of less than 4 words multiple times, being that I want the model to focus on ensuring that the words it predicts are as likely as possible to be grammatically correct, and words closest to the predicted word matter the most, at least that makes sense to me intuitively. There are other ways to do this, but through some trial and error, this actually gave not bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a tokenizer\n",
    "tokenizer = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t')\n",
    "tokenizer.fit_on_texts(poems)\n",
    "total_words = len(tokenizer.word_index)\n",
    "input_sequences = []\n",
    "# iterate through list of lines\n",
    "yy = 0\n",
    "for line in poems:\n",
    "    if yy > 20:\n",
    "        break\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    # keep adding in order subsets of the sequence that start at index 0 as separate sequences.\n",
    "    x = 0\n",
    "    for i in range(1, len(token_list)):\n",
    "        if i < 4:\n",
    "            y = 3\n",
    "        else:\n",
    "            y = 1\n",
    "        for x in range(y):\n",
    "            sequence = token_list[:i+1]\n",
    "            input_sequences.append(sequence)\n",
    "sequence_lengths = [len(x) for x in input_sequences]\n",
    "max_sequence_len = max(sequence_lengths)\n",
    "avg_sequence_len = np.array(sequence_lengths).mean()\n",
    "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "# shuffle just in case\n",
    "np.random.shuffle(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2a92c",
   "metadata": {},
   "source": [
    "## Creating Features and Labels\n",
    "Here I will take the last element of each sequence and use it as the label for that sequence. I will then convert the labels into one hot vectors. Remember, this model is going to be trained to predict the next word given an input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = padded_sequences[:, :-1]\n",
    "labels = padded_sequences[:, -1]\n",
    "one_hot_labels = to_categorical(labels, num_classes = total_words+1)\n",
    "features[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f11c0",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Here, I will construct my model. Because the order of words in an input sequence are extremely important, I will use two adjacent LSTM layers after my embedding layer. I will then use a bunch of Dense layers that scale up to the size of the output layer, which will be the size of the available vocabulary of words. All of these layers will have relu activations. The final layer, as I mentioned above, will have a single 'neuron' for each possible output word, and will have a softmax activation, as we are expecting a single discrete output: the word that we are guessing will be next in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words+1, 100, input_length=max_sequence_len-1),\n",
    "    tf.keras.layers.Conv1D(24, 3),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24)),\n",
    "    tf.keras.layers.Dense(512, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dense(total_words+1, activation = 'softmax')\n",
    "])\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62114991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "history = model.fit(features, one_hot_labels, epochs = 20, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba778ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"I love you very much\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\t# Convert the text into sequences\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t# Pad the sequences\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t# Get the probabilities of predicting a word\n",
    "\tpredicted = model.predict(token_list, verbose=0)\n",
    "\t# Choose the next word based on the maximum probability\n",
    "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
    "\t# Get the actual word from the word index\n",
    "\toutput_word = tokenizer.index_word[predicted]\n",
    "    # Append to the current text\n",
    "\tseed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fafbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
